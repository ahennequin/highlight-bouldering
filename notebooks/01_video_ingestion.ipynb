{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a11e33c",
   "metadata": {},
   "source": [
    "# Retrieve and download the raw Youtube video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12a6d310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytubefix import YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28007b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Janja Garnbret Goes Gold at Paris 2024 ðŸ¥‡ ðŸ‘‘ | Full Replay all climbs\n",
      "avc1.42001E\n",
      "25\n",
      "640 360\n"
     ]
    }
   ],
   "source": [
    "video_link = \"https://www.youtube.com/watch?v=45KmZUc0CzA\"\n",
    "data_folder = \"../data\"\n",
    "\n",
    "\n",
    "yt_item = YouTube(video_link)\n",
    "print(yt_item.title)\n",
    "\n",
    "stream = yt_item.streams.get_highest_resolution()\n",
    "stream.download(output_path=f\"{data_folder}/raw\", filename=f\"{yt_item.title}.mp4\")\n",
    "\n",
    "codec = stream.codecs[0]\n",
    "print(codec)\n",
    "fps = stream.fps\n",
    "print(fps)\n",
    "height, width = stream.width, stream.height\n",
    "print(height, width)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd23791",
   "metadata": {},
   "source": [
    "# Capture highlight moments from the video\n",
    "\n",
    "## Naive and personal annotation\n",
    "My first instinct is to try and select moments from the video that I personally consider as highlight moments, and then try to figure out which aspects from the video can be used to automate the process.\n",
    "\n",
    "Here follows the video highlight moments timestamps:\n",
    "\n",
    "Janja Garnbret Goes Gold at Paris 2024 ðŸ¥‡ ðŸ‘‘ | Full Replay all climbs\n",
    "* 0:59 - 1:07\n",
    "* 1:26 - 1:35\n",
    "* 2:30 - 2:45\n",
    "* 2:52 - 3:00\n",
    "* 4:42 - 5:17\n",
    "* 7:04 - 7:34\n",
    "\n",
    "Specific Olympic Games transisions (Fade-in = FI // Fade-out)\n",
    "* 0:58-0:59 (FI)\n",
    "* 1:06-1:07\n",
    "* 2:51-2:52 (FI)\n",
    "* 3:00-3:01\n",
    "* 4:42-4:43 (FI)\n",
    "* 5:16:5-17\n",
    "\n",
    "## Observations \n",
    "\n",
    "For this particular video, the video edits could be a great clue, as the olympic rings appear during transitions before and after a replay. There is also a specific sound at the beginning of the replay, when the olympic rings appear on the screen. It is important to note that this solution would not generalize well with any other video.\n",
    "\n",
    "A more generalizable solution would be to detect when a single person is being the subject of the frame; we would expect these frames to \"matter more\" and, if there are enough consecutive frames, it would constitute a highlight moment.\n",
    "\n",
    "Another general solution related to bouldering is the use of pose estimation, and to find a way to detect rapid changes in poses. This idea would enhance the person detection idea. The issue is how to determine what a \"rapid change in poses\" actually means.\n",
    "\n",
    "Crowd noise could be an indicator of the relevance of a moment during the video, but this is hard to isolate from the commentators, and I feel that crowd noise was tuned down in the video.\n",
    "\n",
    "---\n",
    "\n",
    "Before going toward coding, I will rate the ideas on specific criterions to help me decide which solution to go to.\n",
    "\n",
    "| Idea | Ease to Develop | Generalization to Other Videos | Expected Relevance |\n",
    "|------|------------------|-------------------------------|--------------------|\n",
    "| Olympic Rings (Sound is optional) | +++ | + | +++ |\n",
    "| Single Person Detection | +++ | +++ | ++ |\n",
    "| Pose Estimation and Rapid Changes | + | +++ | +++ |\n",
    "| Crowd Noise Analysis | + | + | ++ |\n",
    "\n",
    "Considering the project timeline, I will develop the most easy solutions which are the Olympic Rings (visually, and optionally with the sound approach), as well as the Single Person Detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf147964",
   "metadata": {},
   "source": [
    "# Olympic Rings and Sound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6fc9c1",
   "metadata": {},
   "source": [
    "## Olympic Rings video transition detection\n",
    "\n",
    "My first intuition when trying to detect the video transitions is to try and find classic DL Computer Vision techniques. \n",
    "Let's first set the fade-in and fade-out video sequences timestamps, and make a function to extract these video sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0ef8193e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from datetime import time\n",
    "from loguru import logger\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e50f7090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define fade-in and fade-out timestamps and durations\n",
    "fade_in_timestamps = [\n",
    "    time(minute=0, second=58, microsecond=800000),\n",
    "    time(minute=2, second=51, microsecond=250000),\n",
    "    time(minute=4, second=42, microsecond=400000),\n",
    "]\n",
    "fade_in_duration = time(second=1, microsecond=0)\n",
    "\n",
    "fade_out_timestamps = [\n",
    "    time(minute=1, second=6, microsecond=500000),\n",
    "    time(minute=3, second=0, microsecond=300000),\n",
    "    time(minute=5, second=16, microsecond=750000),\n",
    "]\n",
    "fade_out_duration = time(second=1, microsecond=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "97b7b063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_video_sequence(cap: cv2.VideoCapture, start_time: time, duration: time):\n",
    "    \"\"\"Given a video capture object, start time, and duration, retrieves the corresponding video frames.\"\"\"\n",
    "\n",
    "    start_frame = ((start_time.microsecond / 1e6) + start_time.second + start_time.minute * 60) * fps    # fps is defined globally from the stream metadata\n",
    "    end_frame = ((duration.microsecond / 1e6) + duration.second + duration.minute * 60) * fps + start_frame\n",
    "    \n",
    "    frames = []\n",
    "    for frame_num in range(int(start_frame), int(end_frame)):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            logger.warning(f\"Could not read frame {frame_num}. Stopping retrieval.\")\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    \n",
    "    return frames\n",
    "\n",
    "\n",
    "def write_video_sequence(frames, output_path: str, fps: float):\n",
    "    \"\"\"Writes a sequence of video frames to a video file.\n",
    "    Note: output video is saved in .mp4 format using the 'mp4v' codec.\"\"\"\n",
    "    if not frames:\n",
    "        logger.warning(\"No frames to write.\")\n",
    "        return\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Using 'mp4v' codec for .mp4 files\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (height, width)) # width and height are defined globally from the stream metadata\n",
    "    \n",
    "    for frame in frames:\n",
    "        out.write(frame)\n",
    "    \n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "81b1a41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing fade-in sequence for timestamp 00:00:58.800000 with 25 frames.\n",
      "Writing fade-in sequence for timestamp 00:02:51.250000 with 25 frames.\n",
      "Writing fade-in sequence for timestamp 00:04:42.400000 with 25 frames.\n",
      "Writing fade-out sequence for timestamp 00:01:06.500000 with 25 frames.\n",
      "Writing fade-out sequence for timestamp 00:03:00.300000 with 25 frames.\n",
      "Writing fade-out sequence for timestamp 00:05:16.750000 with 25 frames.\n"
     ]
    }
   ],
   "source": [
    "def extract_fade_in_out_sequences(video_path: str) -> tuple[list, list]:\n",
    "    \"\"\"Extracts fade-in and fade-out sequences from the video based on predefined timestamps.\"\"\"\n",
    "\n",
    "    assert Path(video_path).is_file(), f\"Video file {video_path} does not exist.\"\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    fade_in_sequences = []\n",
    "    for timestamp in fade_in_timestamps:\n",
    "        sequence = retrieve_video_sequence(cap, timestamp, fade_in_duration)\n",
    "        print(f\"Writing fade-in sequence for timestamp {timestamp} with {len(sequence)} frames.\")\n",
    "        write_video_sequence(sequence, f\"{data_folder}/sequences/fade_in_{timestamp.minute}_{timestamp.second}.mp4\", fps) # fps is defined globally from the stream metadata\n",
    "        fade_in_sequences.append(sequence)\n",
    "    \n",
    "    fade_out_sequences = []\n",
    "    for timestamp in fade_out_timestamps:\n",
    "        sequence = retrieve_video_sequence(cap, timestamp, fade_out_duration)\n",
    "        print(f\"Writing fade-out sequence for timestamp {timestamp} with {len(sequence)} frames.\")\n",
    "        write_video_sequence(sequence, f\"{data_folder}/sequences/fade_out_{timestamp.minute}_{timestamp.second}.mp4\", fps)  # fps is defined globally from the stream metadata\n",
    "        fade_out_sequences.append(sequence)\n",
    "    \n",
    "    cap.release()\n",
    "\n",
    "    return fade_in_sequences, fade_out_sequences\n",
    "\n",
    "fade_in_sequences, fade_out_sequences = extract_fade_in_out_sequences(f\"{data_folder}/raw/Janja Garnbret Goes Gold at Paris 2024 ðŸ¥‡ ðŸ‘‘ | Full Replay all climbs.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b21a9a",
   "metadata": {},
   "source": [
    "Now that we have the sequences, let's embed them and compare them together. This will tell us how \"alike\" the sequences are, and help us decide a threshold to recognize the sequence.\n",
    "\n",
    "Then, we can then process all video frames and compare them to the mean of the sequences embeddings, and see if the sequences are correctly recognized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "143df9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_video_sequence(frames, model, preprocess, device):\n",
    "    \"\"\"Embed the video sequence using a pre-trained model (e.g., CLIP, TimeSformer).\"\"\"\n",
    "\n",
    "    sequence_input = [ preprocess(Image.fromarray(frame)).to(device) for frame in frames ]\n",
    "    sequence_input = torch.stack(sequence_input)  # Stack frames into a batch tensor\n",
    "\n",
    "    with torch.no_grad():\n",
    "        frames_features =  model.encode_image(sequence_input) # Handle batch processing\n",
    "\n",
    "    return frames_features\n",
    "\n",
    "# Load model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "fade_in_sequences_embeddings = [embed_video_sequence(seq, model, preprocess, device) for seq in fade_in_sequences]\n",
    "fade_out_sequences_embeddings = [embed_video_sequence(seq, model, preprocess, device) for seq in fade_out_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c32dfd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between fade-in sequence 0 and fade-in sequence 0: 1.0\n",
      "Similarity between fade-in sequence 0 and fade-in sequence 1: 0.81201171875\n",
      "Similarity between fade-in sequence 0 and fade-in sequence 2: 0.85498046875\n",
      "Similarity between fade-in sequence 1 and fade-in sequence 0: 0.81201171875\n",
      "Similarity between fade-in sequence 1 and fade-in sequence 1: 1.0\n",
      "Similarity between fade-in sequence 1 and fade-in sequence 2: 0.841796875\n",
      "Similarity between fade-in sequence 2 and fade-in sequence 0: 0.85498046875\n",
      "Similarity between fade-in sequence 2 and fade-in sequence 1: 0.841796875\n",
      "Similarity between fade-in sequence 2 and fade-in sequence 2: 1.0\n",
      "\n",
      "\n",
      "Similarity between fade-out sequence 0 and fade-out sequence 0: 1.0\n",
      "Similarity between fade-out sequence 0 and fade-out sequence 1: 0.884765625\n",
      "Similarity between fade-out sequence 0 and fade-out sequence 2: 0.94775390625\n",
      "Similarity between fade-out sequence 1 and fade-out sequence 0: 0.884765625\n",
      "Similarity between fade-out sequence 1 and fade-out sequence 1: 1.0\n",
      "Similarity between fade-out sequence 1 and fade-out sequence 2: 0.88671875\n",
      "Similarity between fade-out sequence 2 and fade-out sequence 0: 0.94775390625\n",
      "Similarity between fade-out sequence 2 and fade-out sequence 1: 0.88671875\n",
      "Similarity between fade-out sequence 2 and fade-out sequence 2: 1.0\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(embeddings1, embeddings2):\n",
    "    \"\"\"Compute cosine similarity between two sets of embeddings.\"\"\"\n",
    "    similarity = torch.nn.CosineSimilarity(dim=1)(embeddings1, embeddings2)\n",
    "    return similarity\n",
    "\n",
    "# Compare fade-in embeddings between each other\n",
    "for i, fade_in_emb in enumerate(fade_in_sequences_embeddings):\n",
    "    for j, fade_in_emb2 in enumerate(fade_in_sequences_embeddings):\n",
    "        similarity = cosine_similarity(fade_in_emb, fade_in_emb2)\n",
    "        print(f\"Similarity between fade-in sequence {i} and fade-in sequence {j}: {similarity.mean().item()}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Compare fade-out embeddings between each other\n",
    "for i, fade_out_emb in enumerate(fade_out_sequences_embeddings):\n",
    "    for j, fade_out_emb2 in enumerate(fade_out_sequences_embeddings):\n",
    "        similarity = cosine_similarity(fade_out_emb, fade_out_emb2)\n",
    "        print(f\"Similarity between fade-out sequence {i} and fade-out sequence {j}: {similarity.mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7aa7a2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings_from_video(video_path: str) -> torch.Tensor:\n",
    "    \"\"\"Extracts fade-in and fade-out sequences from the video based on predefined timestamps and returns their embeddings.\"\"\"\n",
    "\n",
    "    assert Path(video_path).is_file(), f\"Video file {video_path} does not exist.\"\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    embeddings = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Preprocess the frame and move to device\n",
    "        input_tensor = preprocess(Image.fromarray(frame)).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embedding = model.encode_image(input_tensor)\n",
    "        \n",
    "        embeddings.append(embedding)\n",
    "    \n",
    "    cap.release()\n",
    "\n",
    "    video_embeddings_tensor = torch.cat(embeddings, dim=0)  # Shape: (num_frames, embedding_dim)\n",
    "\n",
    "    return video_embeddings_tensor\n",
    "\n",
    "video_embeddings_tensor = extract_embeddings_from_video(f\"{data_folder}/raw/Janja Garnbret Goes Gold at Paris 2024 ðŸ¥‡ ðŸ‘‘ | Full Replay all climbs.mp4\")\n",
    "torch.save(video_embeddings_tensor, f\"{data_folder}/embeddings/video_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40682be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window starting at frame 1475 is similar to fade-in sequence with confidence 0.86767578125\n",
      "Window starting at frame 4275 is similar to fade-in sequence with confidence 0.8447265625\n",
      "Window starting at frame 4500 is similar to fade-out sequence with confidence 0.8154296875\n",
      "Window starting at frame 7925 is similar to fade-out sequence with confidence 0.810546875\n",
      "Window starting at frame 10600 is similar to fade-in sequence with confidence 0.8505859375\n",
      "Window starting at frame 11350 is similar to fade-out sequence with confidence 0.76123046875\n"
     ]
    }
   ],
   "source": [
    "fade_in_confidence_threshold = .84\n",
    "fade_out_confidence_threshold = .75\n",
    "\n",
    "# Make a sliding window over the video embeddings\n",
    "window_duration = 1 # seconds\n",
    "stride = int(window_duration * fps)  # Number of frames in the window\n",
    "num_frames = video_embeddings_tensor.shape[0]\n",
    "\n",
    "fade_in_events = []\n",
    "fade_out_events = []\n",
    "\n",
    "mean_fade_in_embedding = torch.stack(fade_in_sequences_embeddings).mean(dim=0)\n",
    "mean_fade_out_embedding = torch.stack(fade_out_sequences_embeddings).mean(dim=0)\n",
    "\n",
    "for start in range(0, num_frames - stride + 1, stride):\n",
    "    window_emb = video_embeddings_tensor[start:start+stride]\n",
    "    if cosine_similarity(window_emb, mean_fade_in_embedding).mean() > fade_in_confidence_threshold:\n",
    "        print(f\"Window starting at frame {start} is similar to fade-in sequence with confidence {cosine_similarity(window_emb, mean_fade_in_embedding).mean().item()}\")\n",
    "        fade_in_events.append(start)\n",
    "    if cosine_similarity(window_emb, mean_fade_out_embedding).mean() > fade_out_confidence_threshold:\n",
    "        print(f\"Window starting at frame {start} is similar to fade-out sequence with confidence {cosine_similarity(window_emb, mean_fade_out_embedding).mean().item()}\")\n",
    "        fade_out_events.append(start)\n",
    "\n",
    "# Retrieve and save the associated video sequences for fade-in events\n",
    "cap = cv2.VideoCapture(f\"{data_folder}/raw/Janja Garnbret Goes Gold at Paris 2024 ðŸ¥‡ ðŸ‘‘ | Full Replay all climbs.mp4\")\n",
    "\n",
    "for event in fade_in_events:\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, event)\n",
    "    frames = []\n",
    "    for _ in range(stride):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    write_video_sequence(frames, f\"{data_folder}/detected_events/fade_in_event_{event}.mp4\", fps)\n",
    "\n",
    "# Retrieve and save the associated video sequences for fade-out events\n",
    "cap = cv2.VideoCapture(f\"{data_folder}/raw/Janja Garnbret Goes Gold at Paris 2024 ðŸ¥‡ ðŸ‘‘ | Full Replay all climbs.mp4\")\n",
    "\n",
    "for event in fade_out_events:\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, event)\n",
    "    frames = []\n",
    "    for _ in range(stride):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    write_video_sequence(frames, f\"{data_folder}/detected_events/fade_out_event_{event}.mp4\", fps)\n",
    "\n",
    "for event in fade_in_events:\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, event)\n",
    "    frames = []\n",
    "    for _ in range(stride):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    write_video_sequence(frames, f\"{data_folder}/detected_events/fade_in_event_{event}.mp4\", fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3fa89c",
   "metadata": {},
   "source": [
    "I tweaked the thresholds retrieve exactly the same sequences. Contrary to what I thought, this method was not enough to segment the video sequence from any other in the video.\n",
    "\n",
    "I expect that this may be caused by multiple reasons, such as :\n",
    "* the quality of sequences (the athlete is seen in those, and could inflate the confidence score)\n",
    "* the mean cosine similarity score - one final score is more comprehensible but reduces the data information drastically\n",
    "* the same idea applies for fade-in and fade-out sequences samples, which are meaned altogether frame by frame\n",
    "\n",
    "These results make me think that sequence-to-sequence comparison might be worse than simple image matching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900e4337",
   "metadata": {},
   "source": [
    "# Single-person detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5199a65",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731dc8a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hightlight-bouldering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
