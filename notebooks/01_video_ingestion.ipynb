{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a11e33c",
   "metadata": {},
   "source": [
    "# Retrieve and download the raw Youtube video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "12a6d310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytubefix import YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "28007b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Janja Garnbret Goes Gold at Paris 2024 ðŸ¥‡ ðŸ‘‘ | Full Replay all climbs\n",
      "avc1.42001E\n",
      "25\n",
      "640 360\n"
     ]
    }
   ],
   "source": [
    "video_link = \"https://www.youtube.com/watch?v=45KmZUc0CzA\"\n",
    "data_folder = \"../data\"\n",
    "\n",
    "\n",
    "yt_item = YouTube(video_link)\n",
    "print(yt_item.title)\n",
    "\n",
    "stream = yt_item.streams.get_highest_resolution()\n",
    "stream.download(output_path=f\"{data_folder}/raw\", filename=f\"{yt_item.title}.mp4\")\n",
    "\n",
    "codec = stream.codecs[0]\n",
    "print(codec)\n",
    "fps = stream.fps\n",
    "print(fps)\n",
    "height, width = stream.width, stream.height\n",
    "print(height, width)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd23791",
   "metadata": {},
   "source": [
    "# Capture highlight moments from the video\n",
    "\n",
    "## Naive and personal annotation\n",
    "My first instinct is to try and select moments from the video that I personally consider as highlight moments, and then try to figure out which aspects from the video can be used to automate the process.\n",
    "\n",
    "Here follows the video highlight moments timestamps:\n",
    "\n",
    "Janja Garnbret Goes Gold at Paris 2024 ðŸ¥‡ ðŸ‘‘ | Full Replay all climbs\n",
    "* 0:59 - 1:07\n",
    "* 1:26 - 1:35\n",
    "* 2:30 - 2:45\n",
    "* 2:52 - 3:00\n",
    "* 4:42 - 5:17\n",
    "* 7:04 - 7:34\n",
    "\n",
    "Specific Olympic Games transisions (Fade-in = FI // Fade-out)\n",
    "* 0:58-0:59 (FI)\n",
    "* 1:06-1:07\n",
    "* 2:51-2:52 (FI)\n",
    "* 3:00-3:01\n",
    "* 4:42-4:43 (FI)\n",
    "* 5:16:5-17\n",
    "\n",
    "## Observations \n",
    "\n",
    "For this particular video, the video edits could be a great clue, as the olympic rings appear during transitions before and after a replay. There is also a specific sound at the beginning of the replay, when the olympic rings appear on the screen. It is important to note that this solution would not generalize well with any other video.\n",
    "\n",
    "A more generalizable solution would be to detect when a single person is being the subject of the frame; we would expect these frames to \"matter more\" and, if there are enough consecutive frames, it would constitute a highlight moment.\n",
    "\n",
    "Another general solution related to bouldering is the use of pose estimation, and to find a way to detect rapid changes in poses. This idea would enhance the person detection idea. The issue is how to determine what a \"rapid change in poses\" actually means.\n",
    "\n",
    "Crowd noise could be an indicator of the relevance of a moment during the video, but this is hard to isolate from the commentators, and I feel that crowd noise was tuned down in the video.\n",
    "\n",
    "---\n",
    "\n",
    "Before going toward coding, I will rate the ideas on specific criterions to help me decide which solution to go to.\n",
    "\n",
    "| Idea | Ease to Develop | Generalization to Other Videos | Expected Relevance |\n",
    "|------|------------------|-------------------------------|--------------------|\n",
    "| Olympic Rings (Sound is optional) | +++ | + | +++ |\n",
    "| Single Person Detection | +++ | +++ | ++ |\n",
    "| Pose Estimation and Rapid Changes | + | +++ | +++ |\n",
    "| Crowd Noise Analysis | + | + | ++ |\n",
    "\n",
    "Considering the project timeline, I will develop the most easy solutions which are the Olympic Rings (visually, and optionally with the sound approach), as well as the Single Person Detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf147964",
   "metadata": {},
   "source": [
    "# Olympic Rings and Sound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6fc9c1",
   "metadata": {},
   "source": [
    "## Olympic Rings video transition detection\n",
    "\n",
    "My first intuition when trying to detect the video transitions is to try and find classic DL Computer Vision techniques. \n",
    "Let's first set the fade-in and fade-out video sequences timestamps, and make a function to extract these video sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0ef8193e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from datetime import time\n",
    "from loguru import logger\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e50f7090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define fade-in and fade-out timestamps and durations\n",
    "fade_in_timestamps = [\n",
    "    time(minute=0, second=58, microsecond=500000),\n",
    "    time(minute=2, second=51),\n",
    "    time(minute=4, second=42),\n",
    "]\n",
    "fade_in_duration = time(second=1)\n",
    "\n",
    "fade_out_timestamps = [\n",
    "    time(minute=1, second=6, microsecond=500000),\n",
    "    time(minute=3, second=0, microsecond=500000),\n",
    "    time(minute=5, second=16, microsecond=500000),\n",
    "]\n",
    "fade_out_duration = time(second=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "97b7b063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_video_sequence(cap: cv2.VideoCapture, start_time: time, duration: time):\n",
    "    \"\"\"Given a video capture object, start time, and duration, retrieves the corresponding video frames.\"\"\"\n",
    "\n",
    "    start_frame = (start_time.microsecond / 1e6 + start_time.second + start_time.minute * 60) * fps    # fps is defined globally from the stream metadata\n",
    "    end_frame = (duration.microsecond / 1e6 + duration.second + duration.minute * 60) * fps + start_frame\n",
    "    \n",
    "    frames = []\n",
    "    for frame_num in range(int(start_frame), int(end_frame)):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            logger.warning(f\"Could not read frame {frame_num}. Stopping retrieval.\")\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    \n",
    "    return frames\n",
    "\n",
    "\n",
    "def write_video_sequence(frames, output_path: str, fps: float):\n",
    "    \"\"\"Writes a sequence of video frames to a video file.\n",
    "    Note: output video is saved in .mp4 format using the 'mp4v' codec.\"\"\"\n",
    "    if not frames:\n",
    "        logger.warning(\"No frames to write.\")\n",
    "        return\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Using 'mp4v' codec for .mp4 files\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (height, width)) # width and height are defined globally from the stream metadata\n",
    "    \n",
    "    for frame in frames:\n",
    "        out.write(frame)\n",
    "    \n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "81b1a41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing fade-in sequence for timestamp 00:00:58.500000 with 25 frames.\n",
      "Writing fade-in sequence for timestamp 00:02:51 with 25 frames.\n",
      "Writing fade-in sequence for timestamp 00:04:42 with 25 frames.\n",
      "Writing fade-out sequence for timestamp 00:01:06.500000 with 25 frames.\n",
      "Writing fade-out sequence for timestamp 00:03:00.500000 with 25 frames.\n",
      "Writing fade-out sequence for timestamp 00:05:16.500000 with 25 frames.\n"
     ]
    }
   ],
   "source": [
    "def extract_fade_in_out_sequences(video_path: str):\n",
    "    \"\"\"Extracts fade-in and fade-out sequences from the video based on predefined timestamps.\"\"\"\n",
    "\n",
    "    assert Path(video_path).is_file(), f\"Video file {video_path} does not exist.\"\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    for timestamp in fade_in_timestamps:\n",
    "        sequence = retrieve_video_sequence(cap, timestamp, fade_in_duration)\n",
    "        print(f\"Writing fade-in sequence for timestamp {timestamp} with {len(sequence)} frames.\")\n",
    "        write_video_sequence(sequence, f\"{data_folder}/sequences/fade_in_{timestamp.minute}_{timestamp.second}.mp4\", fps) # fps is defined globally from the stream metadata\n",
    "    \n",
    "    for timestamp in fade_out_timestamps:\n",
    "        sequence = retrieve_video_sequence(cap, timestamp, fade_out_duration)\n",
    "        print(f\"Writing fade-out sequence for timestamp {timestamp} with {len(sequence)} frames.\")\n",
    "        write_video_sequence(sequence, f\"{data_folder}/sequences/fade_out_{timestamp.minute}_{timestamp.second}.mp4\", fps)  # fps is defined globally from the stream metadata\n",
    "    \n",
    "    cap.release()\n",
    "\n",
    "extract_fade_in_out_sequences(f\"{data_folder}/raw/Janja Garnbret Goes Gold at Paris 2024 ðŸ¥‡ ðŸ‘‘ | Full Replay all climbs.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebd92c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hightlight-bouldering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
